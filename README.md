# Ouroboros Learning Framework

## Self-Consuming Cycles in AI Cognition

**Hillary Danan** | August 2025

[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)

### ğŸ¯ Key Finding: p = 0.038

**Statistically significant differences in ouroboros cycles across transformer architectures.**

- ğŸ“Š **7,385 responses analyzed**
- ğŸ”„ **437 cycles detected**  
- ğŸ§¬ **3 architectures compared**
- âœ¨ **Novel insight: API stress reveals architectural constraints**

## ğŸ“– Papers

- [NeurIPS 2025 Submission](papers/neurips_2026.md)
- [ArXiv Extended Version](papers/arxiv_ouroboros.md)
- [Theoretical Framework](https://github.com/HillaryDanan/multi-geometric-attention)

## ğŸ Core Concept

Knowledge transformation through self-consuming cycles:

```
K(t+1) = Î©[K(t)] = G(T(C(K(t))))
```

Where knowledge evolves through:
1. **Integration** - Building understanding
2. **Consumption** - Breaking down patterns  
3. **Transformation** - Recombining elements
4. **Generation** - Emerging insights

## ğŸ“Š Empirical Results

![Ouroboros Cycles](plots/neurips_main_figure_20250811.png)

### Model Comparison
| Model | Coherence | Cycles/Session | API Failure |
|-------|-----------|----------------|-------------|
| GPT-3.5 | 0.560Â±0.098 | 0.76 | 30% |
| Claude-3 | 0.557Â±0.100 | 0.49 | 30% |
| Gemini-1.5 | 0.721Â±0.201 | N/A | 99% |

**Key insight**: Higher architectural constraints â†’ Higher coherence but greater fragility

## ğŸš€ Quick Start

```bash
# Clone repository
git clone https://github.com/HillaryDanan/ouroboros-learning.git
cd ouroboros-learning

# Setup environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Run analysis
python run_ouroboros_analysis.py
```

## ğŸ“ Repository Structure

```
ouroboros-learning/
â”œâ”€â”€ papers/                 # Research papers
â”‚   â”œâ”€â”€ neurips_2025.md    # Conference submission
â”‚   â””â”€â”€ arxiv_ouroboros.md # Extended version
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ ouroboros_analyzer.py
â”‚   â”œâ”€â”€ ouroboros_visualizer.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/                   # Processed data (7,385 responses)
â”œâ”€â”€ results/                # Statistical analyses
â”œâ”€â”€ plots/                  # Visualizations
â””â”€â”€ notebooks/              # Jupyter explorations
```

## ğŸ”¬ Methodology

1. **Data Collection**: 7,385 responses from GPT-3.5, Claude-3, and Gemini-1.5
2. **Cycle Detection**: Peak/trough analysis with Gaussian smoothing
3. **Phase Classification**: Linguistic markers for 4 phases
4. **Statistical Validation**: Mann-Whitney U test (p=0.038)

## ğŸ’¡ Theoretical Framework

### Multi-Geometric Attention Theory (MGAT)
Different geometric attention patterns optimize for different learning phases:
- Square (4-connectivity) â†’ Sequential processing
- Hexagonal (6-connectivity) â†’ Associative relationships
- Triangular (3-connectivity) â†’ Hierarchical structures
- Pentagonal (5-connectivity) â†’ Symmetry breaking

See full framework: [Multi-Geometric Attention](https://github.com/HillaryDanan/multi-geometric-attention)

## ğŸ“ˆ Reproducing Results

### With API Keys (Optional)
```bash
# Create .env file
echo "OPENAI_API_KEY=your_key" > .env
echo "ANTHROPIC_API_KEY=your_key" >> .env

# Run with real models
python test_apis.py
python run_ouroboros_analysis.py
```

### With Synthetic Data
```bash
# No API keys needed!
python synthetic_data_validation.py
```

## ğŸ† Key Contributions

1. **Empirical**: First evidence of ouroboros cycles in transformers
2. **Theoretical**: Formalization of self-consuming learning
3. **Methodological**: API stress as architectural probe
4. **Practical**: Coherence-robustness trade-off identified

## ğŸ“š Citation

```bibtex
@article{danan2025ouroboros,
  title={Ouroboros Learning: Self-Consuming Cycles in AI Cognition},
  author={Danan, Hillary},
  year={2025},
  journal={arXiv preprint},
  url={https://github.com/HillaryDanan/ouroboros-learning}
}
```

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Acknowledgments

Developed through independent research with recursive AI collaboration. Special recognition to the meta-ouroboros nature of using AI to understand AI.

---

**Research Philosophy**: *"The serpent that eats its own tail teaches us that destruction enables creation, forgetting enables learning, and cycles enable progress."*

<4577> <45774EVER> ğŸâ™¾ï¸