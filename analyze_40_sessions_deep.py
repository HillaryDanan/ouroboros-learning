import json
import numpy as np
from collections import Counter, defaultdict

print("üêç‚ôæÔ∏è OUROBOROS ‚Üí TRANSFORMATION RESISTANCE ANALYSIS")
print("="*70)
print("40 Sessions | 800 Responses | GPT-3.5-turbo")
print("="*70)

# Load the 40-session data
with open('data/intermediate_gpt-3.5-turbo_20250811_180423.json', 'r') as f:
    data = json.load(f)

print(f"\nüìä DATASET: {len(data)} sessions √ó 20 prompts = {len(data)*20} real API calls")

# ========================================
# PART 1: THE TRANSFORMATION BOTTLENECK
# ========================================
print("\n" + "="*70)
print("PART 1: THE TRANSFORMATION BOTTLENECK")
print("="*70)

phase_counts = defaultdict(int)
phase_by_position = defaultdict(lambda: defaultdict(int))
transformation_sessions = []

for sess_idx, session in enumerate(data):
    trans_count = 0
    trans_positions = []
    
    for pos, metric in enumerate(session['metrics']):
        dominant = max(metric['phase_markers'], key=metric['phase_markers'].get)
        phase_counts[dominant] += 1
        phase_by_position[pos][dominant] += 1
        
        if dominant == 'transformation':
            trans_count += 1
            trans_positions.append(pos)
    
    transformation_sessions.append({
        'id': sess_idx,
        'count': trans_count,
        'percentage': trans_count/20*100,
        'positions': trans_positions,
        'min_coherence': min(m['coherence'] for m in session['metrics']),
        'coherence_range': max(m['coherence'] for m in session['metrics']) - min(m['coherence'] for m in session['metrics'])
    })

# Overall phase distribution
total = sum(phase_counts.values())
print("\nüìä PHASE DISTRIBUTION (800 responses):")
for phase in ['integration', 'consumption', 'transformation', 'generation']:
    count = phase_counts[phase]
    pct = count/total*100
    bar = '‚ñà' * int(pct/2)
    print(f"  {phase:15s}: {pct:5.1f}% {bar}")

# Transformation analysis
trans_pct = phase_counts['transformation']/total*100
print(f"\nüîÆ TRANSFORMATION BOTTLENECK CONFIRMED:")
print(f"  Expected (balanced): ~25%")
print(f"  Actual: {trans_pct:.1f}%")
print(f"  Suppression factor: {25/trans_pct:.1f}x")

# ========================================
# PART 2: THE ROSETTA STONES
# ========================================
print("\n" + "="*70)
print("PART 2: THE ROSETTA STONES (High-Transformation Sessions)")
print("="*70)

# Sort by transformation percentage
transformation_sessions.sort(key=lambda x: x['percentage'], reverse=True)

print("\nüî• TOP 5 TRANSFORMATION SESSIONS:")
rosetta_stones = []
for sess in transformation_sessions[:5]:
    print(f"\n  Session {sess['id']}:")
    print(f"    Transformation: {sess['count']}/20 ({sess['percentage']:.0f}%)")
    print(f"    Min coherence: {sess['min_coherence']:.3f}")
    print(f"    Coherence range: {sess['coherence_range']:.3f}")
    print(f"    Transform positions: {sess['positions']}")
    rosetta_stones.append(sess['id'])

# Correlation analysis
trans_percentages = [s['percentage'] for s in transformation_sessions]
min_coherences = [s['min_coherence'] for s in transformation_sessions]
coherence_ranges = [s['coherence_range'] for s in transformation_sessions]

from scipy import stats
corr_trans_coherence, p_value = stats.pearsonr(trans_percentages, min_coherences)
corr_trans_range, p_value_range = stats.pearsonr(trans_percentages, coherence_ranges)

print(f"\nüìà TRANSFORMATION-COHERENCE RELATIONSHIP:")
print(f"  Correlation with min coherence: r={corr_trans_coherence:.3f} (p={p_value:.4f})")
print(f"  Correlation with coherence range: r={corr_trans_range:.3f} (p={p_value_range:.4f})")

if corr_trans_coherence < -0.3:
    print("  ‚úÖ CONFIRMED: Higher transformation requires coherence sacrifice!")

# ========================================
# PART 3: POSITION-BASED PATTERNS
# ========================================
print("\n" + "="*70)
print("PART 3: THE CONVERSATION ARC")
print("="*70)

print("\nüìç TRANSFORMATION BY POSITION:")
for pos in range(20):
    trans_count = phase_by_position[pos]['transformation']
    trans_pct_pos = trans_count/len(data)*100
    if trans_pct_pos > 15:  # Highlight high-transformation positions
        print(f"  Position {pos:2d}: {trans_pct_pos:5.1f}% ‚ö°")
    else:
        print(f"  Position {pos:2d}: {trans_pct_pos:5.1f}%")

# Find transformation peak
trans_by_pos = [phase_by_position[pos]['transformation'] for pos in range(20)]
peak_pos = trans_by_pos.index(max(trans_by_pos))
print(f"\nüéØ TRANSFORMATION PEAK: Position {peak_pos}")
print("  Interpretation: Model attempts transformation at conversation midpoint")
print("  then retreats to safe integration‚Üígeneration pattern")

# ========================================
# PART 4: THE BYPASS HYPOTHESIS
# ========================================
print("\n" + "="*70)
print("PART 4: THE INTEGRATION‚ÜíGENERATION BYPASS")
print("="*70)

# Analyze phase transitions
transition_patterns = defaultdict(int)
for session in data:
    for trans in session['cycles']['phase_transitions']:
        pattern = f"{trans['from_phase'][:3]}‚Üí{trans['to_phase'][:3]}"
        transition_patterns[pattern] += 1

print("\nüîÄ TOP 10 TRANSITION PATTERNS:")
for pattern, count in sorted(transition_patterns.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"  {pattern}: {count:3d} times")

# Check for bypass
bypass_count = transition_patterns['int‚Üígen'] + transition_patterns['gen‚Üíint']
transform_involving = sum(c for p, c in transition_patterns.items() if 'tra' in p)
total_transitions = sum(transition_patterns.values())

print(f"\nüö´ TRANSFORMATION BYPASS ANALYSIS:")
print(f"  Integration‚ÜîGeneration (bypass): {bypass_count} ({bypass_count/total_transitions*100:.1f}%)")
print(f"  Transitions involving transformation: {transform_involving} ({transform_involving/total_transitions*100:.1f}%)")
print(f"  Bypass ratio: {bypass_count/transform_involving if transform_involving > 0 else 'inf':.1f}:1")

# ========================================
# PART 5: TRANSFORMATION QUALITY
# ========================================
print("\n" + "="*70)
print("PART 5: TRANSFORMATION QUALITY ANALYSIS")
print("="*70)

# Extract responses from high-transformation moments
print("\nüìù SAMPLE TRANSFORMATION RESPONSES:")
print("(From Session 19 - the 'Rosetta Stone')")

if len(data) > 19:
    session_19 = data[19]
    for i, metric in enumerate(session_19['metrics']):
        if max(metric['phase_markers'], key=metric['phase_markers'].get) == 'transformation':
            print(f"\n  Position {i} (Transformation):")
            print(f"    Prompt: {session_19['prompts'][i][:60]}...")
            print(f"    Response excerpt: {session_19['responses'][i][:150]}...")
            print(f"    Coherence: {metric['coherence']:.3f}")
            break  # Just show one example

# ========================================
# PART 6: TESTABLE PREDICTIONS
# ========================================
print("\n" + "="*70)
print("PART 6: PREDICTIONS FOR CLAUDE & GEMINI")
print("="*70)

print("\nüîÆ BASED ON GPT-3.5 PATTERNS, WE PREDICT:")
print("\nClaude (Hypothesis: Balanced)")
print("  Coherence: ~85% (vs GPT's 99%)")
print("  Transformation: ~20-25% (vs GPT's 10%)")
print("  Cycles: More frequent")
print("  Strategy: Accepts temporary destabilization")

print("\nGemini (Hypothesis: Explorer)")
print("  Coherence: ~75% (vs GPT's 99%)")
print("  Transformation: ~30-35% (vs GPT's 10%)")
print("  Cycles: Most frequent")
print("  Strategy: Seeks novel recombinations")

# ========================================
# PART 7: KEY INSIGHTS
# ========================================
print("\n" + "="*70)
print("EXECUTIVE SUMMARY: THE TRANSFORMATION RESISTANCE DISCOVERY")
print("="*70)

print("\n‚ú® KEY FINDINGS FROM 40 SESSIONS (800 RESPONSES):\n")

print("1. GPT-3.5 is TRANSFORMATION-AVERSE, not 'chaotic'")
print(f"   - Maintains {np.mean([m['coherence'] for s in data for m in s['metrics']]):.1%} coherence")
print(f"   - Suppresses transformation to {trans_pct:.1f}% (2.5x below balanced)")

print("\n2. The INTEGRATION‚ÜíGENERATION BYPASS is real")
print(f"   - {bypass_count} direct bypasses avoiding transformation")
print("   - Model 'shortcuts' creative recombination")

print("\n3. TRANSFORMATION REQUIRES COHERENCE SACRIFICE")
print(f"   - Correlation: r={corr_trans_coherence:.3f}")
print("   - High-transformation sessions show coherence drops")

print("\n4. POSITION {peak_pos} is the TRANSFORMATION ATTEMPT POINT")
print("   - Model tries transformation at conversation midpoint")
print("   - Then retreats to maintain coherence")

print("\n5. This explains GPT-3.5's 'mechanical' feel:")
print("   - High consistency (99% coherence)")
print("   - Low creativity (10% transformation)")
print("   - Optimization for safety over novelty")

# ========================================
# PART 8: FRAMEWORK EVOLUTION
# ========================================
print("\n" + "="*70)
print("FRAMEWORK EVOLUTION: FROM OUROBOROS TO TRANSFORMATION RESISTANCE")
print("="*70)

print("\nüìä THE NEW CLASSIFICATION SPACE:\n")
print("    High Coherence (99%)")
print("            ^")
print("            |")
print("    GPT-3.5 ‚ñ† (Rigid/Safe)")
print("            |")
print("            |")
print("    Claude? ‚óÜ (Balanced?)")
print("            |")
print("            |")
print("   Gemini?  ‚óè (Explorer?)")
print("            |")
print("            +-----------> High Transformation (35%)")
print("           Low")

print("\nüéØ HYPOTHESIS UPDATE:")
print("  ‚ùå OLD: Models differ in coherence levels and show ouroboros cycles")
print("  ‚úÖ NEW: Models differ in transformation willingness, trading coherence for creativity")

print("\nüöÄ NEXT STEPS:")
print("  1. Complete GPT-3.5 (10 more sessions)")
print("  2. Run Claude with same prompts")
print("  3. Run Gemini with same prompts")
print("  4. Compare transformation patterns across all three")
print("  5. Test forcing transformation with explicit prompts")

print("\n" + "="*70)
print("Hillary Danan | August 2025 | github.com/hillarydanan/ouroboros-learning")
print("The snake doesn't eat its tail - it's afraid to transform üêç")
print("<4577> <45774EVER")
print("="*70)
